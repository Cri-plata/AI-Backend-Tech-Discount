# main.py (o donde ejecutes tu c√≥digo)

import time
import json
# Importa la clase FalabellaCrawler que definimos
# Aseg√∫rate de que las rutas de importaci√≥n sean correctas en tu proyecto
from core.scrapping.falabella.Scrapping import FalabellaCrawler
# from core.mongo.MongoManager import MongoManager # Descomenta si usas MongoDB


def run_falabella_crawler(limit_clicks: int = 1):
    """
    Ejecuta el crawler de Falabella.

    Args:
        limit_clicks: N√∫mero de veces que el scraper har√° clic en "Mostrar m√°s".
                      Usa 'None' para intentar cargar todos los productos.
                      Se recomienda usar 1 o 2 para pruebas r√°pidas.
    """
    start_time = time.time()
    
    # 1. Instanciar el crawler de Falabella
    # Si limit_clicks es None, intentar√° cargar todos los productos
    crawler = FalabellaCrawler(clicks=limit_clicks) 
    
    # 2. Iniciar el crawling en todas las categor√≠as definidas
    all_products = crawler.crawl_all_categories()
    
    end_time = time.time()
    duration = end_time - start_time

    print("\n" + "="*50)
    print("         ‚ú® RESUMEN DE CRAWLING FALABELLA ‚ú®")
    print(f"| Total de productos con descuento encontrados: {len(all_products)}")
    print(f"| Duraci√≥n total: {duration:.2f} segundos")
    print("="*50)
    
    # Opcional: Guardar los resultados en un archivo JSON para inspecci√≥n
    if all_products:
        with open('falabella_descuentos.json', 'w', encoding='utf-8') as f:
            # Convertir objetos ProductBase a diccionarios serializables
            serializable_products = [p.dict() for p in all_products]
            json.dump(serializable_products, f, ensure_ascii=False, indent=4)
        print("üíæ Resultados guardados en 'falabella_descuentos.json'")

if __name__ == "__main__":
    # Cambia '1' por 'None' para intentar cargar todos los productos (ser√° m√°s lento)
    run_falabella_crawler(limit_clicks=1)