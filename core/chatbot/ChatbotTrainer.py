import os
import json
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
import logging
from typing import List, Dict
from core.mongo.MongoManager import MongoManager

logger = logging.getLogger(__name__)


class ChatbotTrainer:
    """Entrenador del chatbot con LoRA fine-tuning"""

    def __init__(self, model_name: str = "mistralai/Mistral-7B-Instruct-v0.2"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Configuraci√≥n de LoRA
        self.lora_config = LoraConfig(
            r=16,  # Rank
            lora_alpha=32,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
            lora_dropout=0.05,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )

        # Rutas de guardado
        self.output_dir = "models/chatbot_lora"
        os.makedirs(self.output_dir, exist_ok=True)

        logger.info(f"üíª Dispositivo: {self.device}")

    def load_model(self, load_in_4bit: bool = True):
        """Carga el modelo base con configuraci√≥n para RTX 4070 8GB - VERSI√ìN CORREGIDA"""
        try:
            logger.info(f"üîÑ Cargando modelo {self.model_name}...")

            # Configuraci√≥n para 4-bit quantization
            if load_in_4bit and torch.cuda.is_available():
                from transformers import BitsAndBytesConfig

                bnb_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16
                )

                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    quantization_config=bnb_config,
                    device_map="auto",  # ‚úÖ Deja que Hugging Face maneje el device mapping
                    torch_dtype=torch.float16,
                    trust_remote_code=True
                )
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16,
                    device_map="auto" if torch.cuda.is_available() else None,  # ‚úÖ Auto solo si hay GPU
                    trust_remote_code=True
                )

            # Cargar tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

            # Configurar pad token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

            # Aplicar LoRA
            self.model = get_peft_model(self.model, self.lora_config)

            # Mostrar par√°metros entrenables
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
            total_params = sum(p.numel() for p in self.model.parameters())

            logger.info(f"‚úÖ Modelo cargado correctamente")
            logger.info(
                f"üìä Par√°metros entrenables: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)")

        except Exception as e:
            logger.error(f"‚ùå Error cargando modelo: {e}")
            raise
    def create_training_dataset(self) -> Dataset:
        """Crea dataset de entrenamiento desde la base de datos"""
        try:
            logger.info("üìä Creando dataset de entrenamiento...")

            # Obtener productos de la base de datos
            mongo = MongoManager()
            products = mongo.get_all_products(limit=2000)  # Limitar para entrenamiento

            if not products:
                raise ValueError("No hay productos en la base de datos")

            # Crear conversaciones de ejemplo
            conversations = []

            for product in products:
                conversations.extend(self._create_product_conversations(product))

            # Crear ejemplos generales
            conversations.extend(self._create_general_conversations())

            # Formatear para entrenamiento
            formatted_data = []
            for conv in conversations:
                formatted_text = self._format_conversation(conv)
                formatted_data.append({"text": formatted_text})

            logger.info(f"üìù Dataset creado con {len(formatted_data)} ejemplos")

            return Dataset.from_list(formatted_data)

        except Exception as e:
            logger.error(f"‚ùå Error creando dataset: {e}")
            raise

    def _create_product_conversations(self, product: Dict) -> List[Dict]:
        """Crea conversaciones de ejemplo para un producto"""
        name = product.get('name', 'Producto')
        brand = product.get('brand', 'Sin marca')
        category = product.get('category', 'Sin categor√≠a')
        price = product.get('discount_price_num', product.get('original_price_num', 0))
        discount = product.get('discount_percent', '0%')
        url = product.get('product_url', '')

        conversations = []

        # Conversaci√≥n 1: B√∫squeda directa
        conversations.append({
            "user": f"Busco {category.lower()}",
            "assistant": f"¬°Perfecto! Te recomiendo el {name} de {brand}. Est√° disponible por ${price:,.0f}. " +
                         (f"¬°Tiene {discount} de descuento! " if discount != '0%' else "") +
                         f"¬øTe interesa conocer m√°s detalles de este producto?"
        })

        # Conversaci√≥n 2: Precio espec√≠fico
        if discount != '0%':
            conversations.append({
                "user": f"¬øHay ofertas en {category.lower()}?",
                "assistant": f"¬°S√≠! El {name} de {brand} tiene {discount} de descuento. " +
                             f"Su precio actual es ${price:,.0f}. Es una excelente oportunidad."
            })

        # Conversaci√≥n 3: Marca espec√≠fica
        if brand != 'Sin marca':
            conversations.append({
                "user": f"¬øTienes productos de {brand}?",
                "assistant": f"¬°Por supuesto! Tengo el {name} de {brand} disponible por ${price:,.0f}. " +
                             f"Es un excelente {category.lower()}. ¬øTe gustar√≠a m√°s informaci√≥n?"
            })

        return conversations

    def _create_general_conversations(self) -> List[Dict]:
        """Crea conversaciones generales del chatbot"""
        return [
            {
                "user": "Hola",
                "assistant": "¬°Hola! Soy tu asistente de productos tecnol√≥gicos. Te ayudo a encontrar los mejores productos y ofertas en tiendas de Colombia. ¬øQu√© est√°s buscando hoy?"
            },
            {
                "user": "¬øQu√© productos tienes?",
                "assistant": "Tengo una amplia variedad de productos tecnol√≥gicos: celulares, computadores, tablets, aud√≠fonos, televisores, electrodom√©sticos y mucho m√°s. Todos con precios actualizados y ofertas especiales. ¬øHay algo espec√≠fico que te interese?"
            },
            {
                "user": "¬øHay ofertas?",
                "assistant": "¬°Claro que s√≠! Constantemente actualizo las mejores ofertas y descuentos disponibles. ¬øEn qu√© categor√≠a de productos te gustar√≠a ver ofertas? Puedo mostrarte celulares, computadores, electrodom√©sticos o cualquier otra categor√≠a."
            },
            {
                "user": "¬øC√≥mo funciona esto?",
                "assistant": "Es muy f√°cil: solo dime qu√© producto buscas, tu presupuesto o si quieres ver ofertas especiales. Yo busco en mi base de datos actualizada y te muestro las mejores opciones disponibles en tiendas de Colombia. ¬°Empecemos!"
            },
            {
                "user": "¬øLos precios est√°n actualizados?",
                "assistant": "S√≠, los precios se actualizan regularmente mediante scraping de las principales tiendas de Colombia. Siempre te muestro la informaci√≥n m√°s reciente disponible, incluyendo descuentos y ofertas especiales."
            }
        ]

    def _format_conversation(self, conversation: Dict) -> str:
        """Formatea una conversaci√≥n para el entrenamiento"""
        user_msg = conversation["user"]
        assistant_msg = conversation["assistant"]

        # Formato para Mistral Instruct
        return f"<s>[INST] {user_msg} [/INST] {assistant_msg}</s>"

    def tokenize_function(self, examples):
        """Tokeniza los ejemplos para entrenamiento - VERSI√ìN CORREGIDA"""
        # Tokenizar sin return_tensors para que datasets pueda manejar los arrays
        tokenized = self.tokenizer(
            examples["text"],
            truncation=True,
            padding=True,  # ‚úÖ Cambiado a True
            max_length=512,
            # ‚ùå REMOVED: return_tensors="pt" - Esto causa el error
        )

        # Asegurar que todas las secuencias tengan la misma longitud
        # agregando padding donde sea necesario
        return tokenized

    def train(self, dataset: Dataset, epochs: int = 3, batch_size: int = 4):
        """Entrena el modelo con LoRA - VERSI√ìN CORREGIDA"""
        try:
            logger.info("üöÄ Iniciando entrenamiento...")

            # Tokenizar dataset (sin return_tensors)
            tokenized_dataset = dataset.map(self.tokenize_function, batched=True)

            # Configuraci√≥n de entrenamiento
            training_args = TrainingArguments(
                output_dir=self.output_dir,
                num_train_epochs=epochs,
                per_device_train_batch_size=batch_size,
                gradient_accumulation_steps=4,
                warmup_steps=100,
                learning_rate=2e-4,
                fp16=True,
                logging_steps=10,
                save_steps=500,
                save_total_limit=2,
                remove_unused_columns=True,  # ‚úÖ Cambiado a True
                dataloader_drop_last=True,
                gradient_checkpointing=True,
            )

            # Data collator con padding din√°mico
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False,
                pad_to_multiple_of=8,  # ‚úÖ Para mejor rendimiento en GPU
            )

            # Crear trainer
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=tokenized_dataset,
                data_collator=data_collator,
            )

            # Entrenar
            trainer.train()

            # Guardar modelo
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)

            logger.info(f"‚úÖ Entrenamiento completado. Modelo guardado en: {self.output_dir}")

        except Exception as e:
            logger.error(f"‚ùå Error durante entrenamiento: {e}")
            raise

    def save_training_config(self, config: Dict):
        """Guarda la configuraci√≥n del entrenamiento"""
        config_file = os.path.join(self.output_dir, "training_config.json")
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)

        logger.info(f"üíæ Configuraci√≥n guardada en: {config_file}")
